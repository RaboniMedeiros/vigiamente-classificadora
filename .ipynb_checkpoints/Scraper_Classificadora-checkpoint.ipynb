{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa1caaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs de Usuários: ['66772a2c3e11c4f746ec936b', '66772a2c3e11c4f746ec936b', '66772a2c3e11c4f746ec936b', '66772a2c3e11c4f746ec936b', '66772a2c3e11c4f746ec936b', '6678c37f5285516dbef87718']\n",
      "Nomes de Perfis: ['Germancanoofi', 'jhonariasa', 'fredgol9', 'phganso', 'tsilva3', 'vigiamenteteste']\n",
      "Perfis: ['Germancanoofi', 'jhonariasa', 'fredgol9', 'phganso', 'tsilva3', 'vigiamenteteste']\n"
     ]
    }
   ],
   "source": [
    "from ntscraper import Nitter\n",
    "import requests\n",
    "import json\n",
    "import random\n",
    "\n",
    "# URL da sua API\n",
    "login_url = 'http://localhost:3000/auth/login'\n",
    "url = 'http://localhost:3000/usuarios'\n",
    "\n",
    "# Credenciais do usuário\n",
    "credentials = {\n",
    "    'usuario': 'admin',\n",
    "    'senha': 'admin'\n",
    "}\n",
    "\n",
    "# Fazendo uma solicitação POST para obter o token JWT\n",
    "login_response = requests.post(login_url, json=credentials)\n",
    "\n",
    "if login_response.status_code == 200:\n",
    "    token = login_response.json()['token']\n",
    "    \n",
    "    # Incluindo o token JWT no cabeçalho Authorization\n",
    "    headers = {\n",
    "        'Authorization': \"Bearer \" + token\n",
    "    }\n",
    "\n",
    "    # Fazendo uma solicitação GET para a API protegida\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Verificando se a solicitação foi bem-sucedida (código de status 200)\n",
    "    if response.status_code == 200:\n",
    "        # Se a solicitação foi bem-sucedida, você pode acessar os dados da resposta assim:\n",
    "        data = response.json()  # Isso assume que sua API retorna dados no formato JSON\n",
    "        \n",
    "        # Inicializa listas vazias para armazenar os nomes de perfil, ids de usuário e nomes de perfil\n",
    "        perfis = []\n",
    "        ids_usuarios = []\n",
    "        nomes_perfis = []\n",
    "\n",
    "        # Percorre a resposta da API\n",
    "        for usuario in data:\n",
    "            # Itera sobre os perfis do usuário\n",
    "            for perfil in usuario.get('perfis', []):\n",
    "                ids_usuarios.append(usuario['_id'])\n",
    "                nomes_perfis.append(perfil['usuario'])\n",
    "                perfis.append(perfil['usuario'])\n",
    "\n",
    "        # Imprime as listas\n",
    "        print(\"IDs de Usuários:\", ids_usuarios)\n",
    "        print(\"Nomes de Perfis:\", nomes_perfis)\n",
    "        print(\"Perfis:\", perfis)\n",
    "    else:\n",
    "        print(\"Erro ao acessar a API. Código de status:\", response.status_code)\n",
    "else:\n",
    "    print(\"Erro ao fazer login. Código de status:\", login_response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b1752a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-Jun-24 17:20:19 - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Testing instances:  92%|██████████████████████████████████████████████████████████     | 71/77 [02:12<00:08,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-Jun-24 17:22:37 - Certificate did not match expected hostname: nt.ggtyler.dev. Certificate: {'subject': ((('commonName', '4g.ggtyler.dev'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', \"Let's Encrypt\"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '03A4459CE6DA31CFF555DB78FF3F35BFEA88', 'notBefore': 'May 14 10:55:44 2024 GMT', 'notAfter': 'Aug 12 10:55:43 2024 GMT', 'subjectAltName': (('DNS', '4g.ggtyler.dev'),), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Testing instances:  94%|██████████████████████████████████████████████████████████▉    | 72/77 [02:12<00:05,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-Jun-24 17:22:38 - Certificate did not match expected hostname: nitter.uni-sonia.com. Certificate: {'subject': ((('commonName', '*.xserver.jp'),),), 'issuer': ((('countryName', 'JP'),), (('organizationName', 'CloudSecure Corporation'),), (('commonName', 'CloudSecure RSA Domain Validation Secure Server CA 2'),)), 'version': 3, 'serialNumber': 'ACA67AD2030638EE2DCE8E845B8299A6', 'notBefore': 'Mar 11 00:00:00 2024 GMT', 'notAfter': 'Apr 11 23:59:59 2025 GMT', 'subjectAltName': (('DNS', '*.xserver.jp'), ('DNS', 'xserver.jp')), 'OCSP': ('http://ocsp.sectigo.com',), 'caIssuers': ('http://crt.sectigo.com/CloudSecureRSADomainValidationSecureServerCA2.crt',)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing instances:  99%|██████████████████████████████████████████████████████████████▏| 76/77 [02:18<00:01,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-Jun-24 17:22:45 - Certificate did not match expected hostname: nitter.tinfoil-hat.net. Certificate: {'subject': ((('commonName', 'jelly.tinfoil-hat.de'),),), 'issuer': ((('countryName', 'US'),), (('organizationName', \"Let's Encrypt\"),), (('commonName', 'R3'),)), 'version': 3, 'serialNumber': '044FDE3E7089FB997C3D8AFDE2412CE51554', 'notBefore': 'May 15 09:29:23 2024 GMT', 'notAfter': 'Aug 13 09:29:22 2024 GMT', 'subjectAltName': (('DNS', 'jelly.tinfoil-hat.de'),), 'OCSP': ('http://r3.o.lencr.org',), 'caIssuers': ('http://r3.i.lencr.org/',)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing instances: 100%|███████████████████████████████████████████████████████████████| 77/77 [02:20<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-Jun-24 17:22:53 - Current stats for Germancanoofi: 1 tweets, 0 threads...\n",
      "Tweet inserido com sucesso!\n",
      "ID do tweet inserido: 6678841d479ed5327c41099b\n",
      "23-Jun-24 17:22:55 - Fetching error: Instance has been rate limited.Use another instance or try again later.\n",
      "Nenhum tweet encontrado usando a instância https://nitter.lucabased.xyz\n",
      "23-Jun-24 17:23:04 - Current stats for jhonariasa: 1 tweets, 0 threads...\n",
      "Erro ao inserir tweet: {\"message\":\"O tweet não foi cadastrado porque o link já existe\"}\n",
      "23-Jun-24 17:23:05 - Fetching error: Instance has been rate limited.Use another instance or try again later.\n",
      "Nenhum tweet encontrado usando a instância https://nitter.lucabased.xyz\n",
      "23-Jun-24 17:23:14 - Current stats for fredgol9: 1 tweets, 0 threads...\n",
      "Erro ao inserir tweet: {\"message\":\"O tweet não foi cadastrado porque o link já existe\"}\n",
      "23-Jun-24 17:23:17 - Fetching error: Instance has been rate limited.Use another instance or try again later.\n",
      "Nenhum tweet encontrado usando a instância https://nitter.privacydev.net\n",
      "23-Jun-24 17:23:20 - Fetching error: Instance has been rate limited.Use another instance or try again later.\n",
      "Nenhum tweet encontrado usando a instância https://nitter.lucabased.xyz\n",
      "Erro ao obter tweets para o perfil phganso: Não foi possível obter tweets de nenhuma instância.\n",
      "23-Jun-24 17:23:24 - Empty page on https://nitter.privacydev.net\n",
      "Nenhum tweet encontrado usando a instância https://nitter.privacydev.net\n",
      "23-Jun-24 17:23:33 - Current stats for tsilva3: 1 tweets, 0 threads...\n",
      "Erro ao inserir tweet: {\"message\":\"O tweet não foi cadastrado porque o link já existe\"}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import requests\n",
    "import json\n",
    "import random\n",
    "import ntscraper\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Baixar recursos adicionais do NLTK\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Carregar e treinar o modelo de classificação\n",
    "data = pd.read_excel('Expanded_Dataset_Tweets.xlsx')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "data['processed_tweets'] = data['tweets'].apply(preprocess_text)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['processed_tweets'], data['classificador'], test_size=0.55, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__max_df': [0.8, 0.9, 1.0],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'clf__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Função de scraping\n",
    "instances = [\n",
    "    \"https://nitter.lucabased.xyz\",\n",
    "    \"https://nitter.privacydev.net\"\n",
    "]\n",
    "\n",
    "def get_tweets_with_instance(scraper, username, number, instances):\n",
    "    for _ in range(len(instances)):\n",
    "        instance = random.choice(instances)\n",
    "        try:\n",
    "            tweets = scraper.get_tweets(username, mode=\"user\", number=number, instance=instance)\n",
    "            if tweets and tweets.get('tweets'):\n",
    "                return tweets\n",
    "            else:\n",
    "                print(f\"Nenhum tweet encontrado usando a instância {instance}\")\n",
    "                instances.remove(instance)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao usar a instância {instance}: {e}\")\n",
    "            instances.remove(instance)\n",
    "    raise Exception(\"Não foi possível obter tweets de nenhuma instância.\")\n",
    "\n",
    "scraper = ntscraper.Nitter()\n",
    "\n",
    "# Autenticação na API\n",
    "login_url = 'http://localhost:3000/auth/login'\n",
    "credentials = {\n",
    "    'usuario': 'admin',\n",
    "    'senha': 'admin'\n",
    "}\n",
    "\n",
    "login_response = requests.post(login_url, json=credentials)\n",
    "token = login_response.json()['token']\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': \"Bearer \" + token\n",
    "}\n",
    "\n",
    "# URL da API\n",
    "api_url = \"http://localhost:3000\"\n",
    "# Função principal\n",
    "def process_and_send_tweets(ids_usuarios, nomes_perfis):\n",
    "    for usuarioId, nomePerfil in zip(ids_usuarios, nomes_perfis):\n",
    "        try:\n",
    "            tweets = get_tweets_with_instance(scraper, nomePerfil, 1, instances.copy())\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao obter tweets para o perfil {nomePerfil}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        tweets_processados = []\n",
    "        for tweet in tweets['tweets']:\n",
    "            tweet_text = tweet['text']\n",
    "            processed_text = preprocess_text(tweet_text)\n",
    "            is_suicida = best_pipeline.predict([processed_text])[0] == 'Post  com Potencial Suicida'\n",
    "            \n",
    "            tweet_processado = {\n",
    "                'link': tweet['link'],\n",
    "                'texto': tweet_text,\n",
    "                'usuarioId': usuarioId,\n",
    "                'nomePerfil': nomePerfil,\n",
    "                'isSuicida': is_suicida\n",
    "            }\n",
    "            tweets_processados.append(tweet_processado)\n",
    "\n",
    "        for tweet in tweets_processados:\n",
    "            dados = json.dumps(tweet)\n",
    "            response = requests.post(f\"{api_url}/tweets\", data=dados, headers=headers)\n",
    "            if response.status_code == 201:\n",
    "                print(\"Tweet inserido com sucesso!\")\n",
    "                id_inserido = response.json()['tweet']['_id']\n",
    "                print(\"ID do tweet inserido:\", id_inserido)\n",
    "            else:\n",
    "                print(\"Erro ao inserir tweet:\", response.text)\n",
    "\n",
    "# Executar o processo\n",
    "process_and_send_tweets(ids_usuarios, nomes_perfis)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
