{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff0e6639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             precision    recall  f1-score   support\n",
      "\n",
      "Post  com Potencial Suicida       0.91      0.93      0.92       182\n",
      "           Post Não Suicida       0.94      0.92      0.93       211\n",
      "\n",
      "                   accuracy                           0.93       393\n",
      "                  macro avg       0.93      0.93      0.93       393\n",
      "               weighted avg       0.93      0.93      0.93       393\n",
      "\n",
      "Tweet 663d79faa978d3f9816d889f atualizado com sucesso.\n",
      "Tweet 6640df8b3cfbb49b44d413a9 atualizado com sucesso.\n",
      "Tweet 66417f5b3cfbb49b44d41413 atualizado com sucesso.\n",
      "Tweet 66417f5b3cfbb49b44d41414 atualizado com sucesso.\n",
      "Tweet 66417f613cfbb49b44d41418 atualizado com sucesso.\n",
      "Tweet 66417f613cfbb49b44d41417 atualizado com sucesso.\n",
      "Tweet 664180103cfbb49b44d4141b atualizado com sucesso.\n",
      "Tweet 664180173cfbb49b44d4141d atualizado com sucesso.\n",
      "Tweet 664182133cfbb49b44d41433 atualizado com sucesso.\n",
      "Tweet 664182193cfbb49b44d41435 atualizado com sucesso.\n",
      "Tweet 664183093cfbb49b44d41437 atualizado com sucesso.\n",
      "Tweet 6641830f3cfbb49b44d4143d atualizado com sucesso.\n",
      "Tweet 664184623cfbb49b44d41443 atualizado com sucesso.\n",
      "Tweet 664184673cfbb49b44d41448 atualizado com sucesso.\n",
      "Tweet 664184da3cfbb49b44d4144d atualizado com sucesso.\n",
      "Tweet 664185213cfbb49b44d41451 atualizado com sucesso.\n",
      "Tweet 664185643cfbb49b44d41455 atualizado com sucesso.\n",
      "Tweet 664185b03cfbb49b44d41459 atualizado com sucesso.\n",
      "Tweet 664185b53cfbb49b44d4145d atualizado com sucesso.\n",
      "Tweet 6641861a3cfbb49b44d41461 atualizado com sucesso.\n",
      "Tweet 6641882d3cfbb49b44d41465 atualizado com sucesso.\n",
      "Tweet 664188333cfbb49b44d41467 atualizado com sucesso.\n",
      "Tweet 664fd50a3b61b1036b5de0e7 atualizado com sucesso.\n",
      "Tweet 664fd5243b61b1036b5de0e9 atualizado com sucesso.\n",
      "Tweet 664fd5b13b61b1036b5de0eb atualizado com sucesso.\n",
      "Tweet 664fd64f78b8d2784ac4a750 atualizado com sucesso.\n",
      "Tweet 664fd89f599762746f18b9fc atualizado com sucesso.\n",
      "Tweet 664fe05fc873721ee62f8ffb atualizado com sucesso.\n",
      "Tweet 664fe05fc873721ee62f9002 atualizado com sucesso.\n",
      "Tweet 664fe060c873721ee62f900a atualizado com sucesso.\n",
      "Tweet 664fe060c873721ee62f9013 atualizado com sucesso.\n",
      "Tweet 664fe061c873721ee62f901d atualizado com sucesso.\n",
      "Tweet 664fe062c873721ee62f9028 atualizado com sucesso.\n",
      "Tweet 664fe062c873721ee62f9035 atualizado com sucesso.\n",
      "Tweet 664fe063c873721ee62f9042 atualizado com sucesso.\n",
      "Tweet 6658f612e0170bf5ead28db4 atualizado com sucesso.\n",
      "Tweet 6658f618e0170bf5ead28db9 atualizado com sucesso.\n",
      "Tweet 6658f61ee0170bf5ead28dbf atualizado com sucesso.\n",
      "Tweet 665bbb063310fa58ab02b7d5 atualizado com sucesso.\n",
      "Tweet 665bbb063310fa58ab02b7db atualizado com sucesso.\n",
      "Tweet 665bbb073310fa58ab02b7e2 atualizado com sucesso.\n",
      "Tweet 665bbb083310fa58ab02b7ea atualizado com sucesso.\n",
      "Tweet 665bbb083310fa58ab02b7f3 atualizado com sucesso.\n",
      "Tweet 665bbb093310fa58ab02b7fd atualizado com sucesso.\n",
      "Tweet 665bbb093310fa58ab02b808 atualizado com sucesso.\n",
      "Tweet 665bbb0a3310fa58ab02b814 atualizado com sucesso.\n",
      "Tweet 665bbb0b3310fa58ab02b821 atualizado com sucesso.\n",
      "Tweet 665bbb0b3310fa58ab02b82f atualizado com sucesso.\n",
      "Tweet 665bbb0c3310fa58ab02b83e atualizado com sucesso.\n",
      "Tweet 665bbb0c3310fa58ab02b84e atualizado com sucesso.\n",
      "Tweet 665bbb0d3310fa58ab02b85f atualizado com sucesso.\n",
      "Tweet 665bbb133310fa58ab02b872 atualizado com sucesso.\n",
      "Tweet 665bbb153310fa58ab02b88b atualizado com sucesso.\n",
      "Tweet 665bbb163310fa58ab02b89c atualizado com sucesso.\n",
      "Tweet 665bbb173310fa58ab02b8ae atualizado com sucesso.\n",
      "Tweet 665bbb173310fa58ab02b8c1 atualizado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Baixar recursos adicionais do NLTK\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Carregar os dados\n",
    "data = pd.read_excel('Dataset_Tweets.xlsx')\n",
    "\n",
    "# Pré-processamento do texto\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "data['processed_tweets'] = data['tweets'].apply(preprocess_text)\n",
    "\n",
    "# Dividir os dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['processed_tweets'], data['classificador'], test_size=0.45, random_state=42)\n",
    "\n",
    "# Criar o pipeline de classificação\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "# Treinar o modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Avaliar o modelo\n",
    "predictions = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# URL da sua API\n",
    "login_url = 'http://localhost:3000/auth/login'\n",
    "# Credenciais do usuário\n",
    "credentials = {\n",
    "    'usuario': 'admin',\n",
    "    'senha': 'admin'\n",
    "}\n",
    "\n",
    "# Fazendo uma solicitação POST para obter o token JWT\n",
    "login_response = requests.post(login_url, json=credentials)\n",
    "token = login_response.json()['login']\n",
    "# Incluindo o token JWT no cabeçalho Authorization\n",
    "headers = {\n",
    "    'Authorization': \"Bearer \"+token\n",
    "}\n",
    "\n",
    "# Função para classificar tweets e atualizar no banco de dados\n",
    "def classify_and_update_tweets(api_url):\n",
    "    # Obter tweets da API\n",
    "    response = requests.get(f\"{api_url}/tweets\", headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            tweets = response.json()\n",
    "        except ValueError:\n",
    "            print(\"Erro ao decodificar JSON. Resposta não está no formato JSON.\")\n",
    "            return\n",
    "        \n",
    "        for tweet in tweets:\n",
    "            tweet_text = tweet['texto']  # Ajuste este campo conforme a estrutura do seu JSON\n",
    "            processed_text = preprocess_text(tweet_text)\n",
    "            is_suicida = pipeline.predict([processed_text])[0] == 'Post  com Potencial Suicida'\n",
    "            \n",
    "            # Atualizar o campo isSuicida no tweet\n",
    "            tweet_id = tweet['_id']  # Ajuste este campo conforme a estrutura do seu JSON\n",
    "            update_url = f\"{api_url}/tweets/{tweet_id}\"\n",
    "            update_data = {'isSuicida': is_suicida}\n",
    "            update_response = requests.put(update_url, json=update_data, headers=headers)\n",
    "            \n",
    "            if update_response.status_code == 200:\n",
    "                print(f\"Tweet {tweet_id} atualizado com sucesso.\")\n",
    "            else:\n",
    "                print(f\"Erro ao atualizar o tweet {tweet_id}. Código de status:\", update_response.status_code)\n",
    "    else:\n",
    "        print(\"Erro ao acessar a API. Código de status:\", response.status_code)\n",
    "\n",
    "# Definir URL da API\n",
    "api_url = \"http://localhost:3000\"\n",
    "\n",
    "# Classificar e atualizar tweets\n",
    "classify_and_update_tweets(api_url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
